{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides code to test all models with validation datasets (either held out test sets or external validation datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements \n",
    "import sys\n",
    "sys.path.insert(1, './BioAutoMATED/main_classes/')\n",
    "sys.path.append('./BioAutoMATED')\n",
    "from wrapper import run_bioautomated\n",
    "from integrated_design_helpers import *\n",
    "from generic_automl_classes import convert_generic_input, read_in_data_file\n",
    "from generic_deepswarm import print_summary\n",
    "from transfer_learning_helpers import transform_classification_target, transform_regression_target, fit_final_deepswarm_model\n",
    "from generic_tpot import reformat_data_traintest\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.stats as sp\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import autokeras\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for col in [\"Gene\",\"Promoter\",\"RBS\"]:\n",
    "    for i in range(1,6):\n",
    "        data_folder = './dataset/rigorous/'\n",
    "        data_file = f'test_{col}_group{i}.csv'\n",
    "        root_path = f\"./ckpt/rigorous/{col}_group{i}\"\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CGCGCCTTGACGGCTAGCTCAGTCCTAGGTATTGTGCTAGCCGTCG...</td>\n",
       "      <td>11.688265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CGCGCCAAAAAGAGTATTGACTTCGCATCTTTTTGTACCCATAATT...</td>\n",
       "      <td>12.008913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CGCGCCTTGACATAAAGTCTAACCTATAGGTATAATGTGTGGATCT...</td>\n",
       "      <td>9.565730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CGCGCCTTGACAATTAATCATCCGGCTCGTATAATGTGTGGAATTG...</td>\n",
       "      <td>11.556572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CGCGCCTTGACGGCTAGCTCAGTCCTAGGTACAGTGCTAGCTTAAT...</td>\n",
       "      <td>9.913603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>CGCGCCTTTATAGCTAGCTCAGCCCTTGGTACAATGCTAGCGCCTG...</td>\n",
       "      <td>9.061063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>CGCGCCTTGACATTTATCCCTTGCGGCGATATAATGTGTGGATAAG...</td>\n",
       "      <td>12.504289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>CGCGCCTTGACATAAAGTCTAACCTATAGGCATAATTATTTCATCC...</td>\n",
       "      <td>10.204993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>CGCGCCTTGACAGCTAGCTCAGTCCTAGGTATAATGCTAGCACGAA...</td>\n",
       "      <td>10.430272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>CGCGCCTTGACATCGCATCTTTTTGTACCTATAATGTGTGGATAGA...</td>\n",
       "      <td>10.774856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   seq     target\n",
       "0    CGCGCCTTGACGGCTAGCTCAGTCCTAGGTATTGTGCTAGCCGTCG...  11.688265\n",
       "1    CGCGCCAAAAAGAGTATTGACTTCGCATCTTTTTGTACCCATAATT...  12.008913\n",
       "2    CGCGCCTTGACATAAAGTCTAACCTATAGGTATAATGTGTGGATCT...   9.565730\n",
       "3    CGCGCCTTGACAATTAATCATCCGGCTCGTATAATGTGTGGAATTG...  11.556572\n",
       "4    CGCGCCTTGACGGCTAGCTCAGTCCTAGGTACAGTGCTAGCTTAAT...   9.913603\n",
       "..                                                 ...        ...\n",
       "96   CGCGCCTTTATAGCTAGCTCAGCCCTTGGTACAATGCTAGCGCCTG...   9.061063\n",
       "97   CGCGCCTTGACATTTATCCCTTGCGGCGATATAATGTGTGGATAAG...  12.504289\n",
       "98   CGCGCCTTGACATAAAGTCTAACCTATAGGCATAATTATTTCATCC...  10.204993\n",
       "99   CGCGCCTTGACAGCTAGCTCAGTCCTAGGTATAATGCTAGCACGAA...  10.430272\n",
       "100  CGCGCCTTGACATCGCATCTTTTTGTACCTATAATGTGTGGATAGA...  10.774856\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata = pd.read_csv(data_folder+data_file)\n",
    "# rawdata.at[0, 'Seq'] = rawdata.iloc[0].Seq*5\n",
    "# rawdata.Seq = rawdata.Seq.apply(lambda x:x[:826])\n",
    "rawdata.to_csv('./output/experimental_data_fineturn.csv', index=False, encoding = 'utf_8_sig')\n",
    "rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def calculate_metrics(preds, y):\n",
    "    \"\"\"\n",
    "    Calculate 'R2', 'Pearson', and 'Spearman' metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - preds: Predicted values\n",
    "    - y: True values\n",
    "\n",
    "    Returns:\n",
    "    - r2: R-squared (R2) score\n",
    "    - pearson: Pearson correlation coefficient\n",
    "    - spearman: Spearman rank correlation coefficient\n",
    "    \"\"\"\n",
    "    # R-squared (R2) score\n",
    "    r2 = r2_score(y, preds)\n",
    "\n",
    "    # Pearson correlation coefficient\n",
    "    pearson, _ = pearsonr(preds, y)\n",
    "\n",
    "    # Spearman rank correlation coefficient\n",
    "    spearman, _ = spearmanr(preds, y)\n",
    "\n",
    "    return r2, pearson, spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Transfer Learning on a DeepSwarm Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "1710078614.194792 (InputLaye (None, 248, 4, 1)         0         \n",
      "_________________________________________________________________\n",
      "1710078614.195567 (Conv2D)   (None, 248, 4, 8)         208       \n",
      "_________________________________________________________________\n",
      "1710078614.2090578 (Flatten) (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "1710078614.2131774 (Dense)   (None, 1)                 7937      \n",
      "=================================================================\n",
      "Total params: 8,145\n",
      "Trainable params: 8,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "model is originally trainable: True\n",
      "number of layers in the model: 4\n",
      "0: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f4229c4e150>, setting trainable to False\n",
      "1: <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4229c53b50>, setting trainable to False\n",
      "2: <tensorflow.python.keras.layers.core.Flatten object at 0x7f4229c4e8d0>, keeping trainable = True\n",
      "3: <tensorflow.python.keras.layers.core.Dense object at 0x7f4229c4e990>, keeping trainable = True\n"
     ]
    }
   ],
   "source": [
    "# Load DeepSwarm Model and freeze all except last two layers (randomly chose this - feel free to customize)\n",
    "final_model_path = f'{root_path}/outputs/deepswarm/regression/'\n",
    "final_model_name = 'deepswarm_deploy_model.h5'\n",
    "# get sequences with help from https://stackoverflow.com/questions/53183865/unknown-initializer-glorotuniform-when-loading-keras-model\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform(), 'BatchNormalizationV1': BatchNormalization()}): # , 'BatchNormalizationV1': BatchNormalization()\n",
    "    model = tf.keras.models.load_model(final_model_path + final_model_name)\n",
    "print(model.summary())\n",
    "print('model is originally trainable: ' + str(model.trainable))\n",
    "print('number of layers in the model: ' + str(len(model.layers)))\n",
    "\n",
    "# set all layers except last two dense ones to be fixed\n",
    "for layer_idx, layer in enumerate(model.layers):\n",
    "    if layer_idx > len(model.layers) - 3:\n",
    "        print(str(layer_idx) + ': ' + str(layer) + ', keeping trainable = ' + str(layer.trainable))\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        print(str(layer_idx) + ': ' + str(layer) + ', setting trainable to ' + str(layer.trainable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# Transform the test set RBS data to fine-tune this model\n",
    "data_folder = './output/'\n",
    "data_file = 'experimental_data_fineturn.csv'\n",
    "\n",
    "# Give inputs for data generation\n",
    "input_col = 'seq'\n",
    "target_col = 'target'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'regression'\n",
    "model_type = 'deepswarm'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# transform output (target) into bins for classification\n",
    "transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "    \n",
    "# now, we have completed the pre-processing needed to feed our data into deepswarm\n",
    "# deepswarm input: numerical_data_input\n",
    "# deepswarm output: transformed_output\n",
    "X = numerical_data_input\n",
    "y = transformed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.22676253116173395\n",
      "Pearson: [0.66088369]\n",
      "Spearman: 0.6971074464428115\n"
     ]
    }
   ],
   "source": [
    "# 使用微调前的模型进行预测\n",
    "preds = model.predict(X)\n",
    "r2, pearson, spearman = calculate_metrics(preds, y)\n",
    "result_list.append([r2,pearson[0],spearman])\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Transfer Learning on an AutoKeras Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "data_folder = './output/'\n",
    "data_file = 'experimental_data_fineturn.csv'\n",
    "\n",
    "\n",
    "# Give inputs for data generation\n",
    "input_col = 'seq'\n",
    "target_col = 'target'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'regression'\n",
    "model_type = 'autokeras'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# Format data inputs appropriately for autoML platform\n",
    "transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "\n",
    "# now, we have completed the pre-processing needed to feed our data into autokeras\n",
    "# autokeras input: oh_data_input\n",
    "# autokeras output: transformed_output\n",
    "X = oh_data_input\n",
    "y = transformed_output # don't convert to categorical for autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = f'{root_path}/models/autokeras/regression/'\n",
    "final_model_name = 'optimized_autokeras_pipeline_regression.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared after no retraining:  0.01829332498048042\n",
      "Evaluation after no retraining:  0.3801518752715207\n",
      "R2: 0.01829332498048042\n",
      "Pearson: 0.367636133405826\n",
      "Spearman: 0.38268602995511697\n"
     ]
    }
   ],
   "source": [
    "clf = autokeras.utils.pickle_from_file(final_model_path+final_model_name)\n",
    "preds = clf.predict(np.array(X))\n",
    "r2 = r2_score(np.array(y), preds)\n",
    "print(\"R-squared after no retraining: \", r2)\n",
    "print('Evaluation after no retraining: ', evaluation)\n",
    "r2, pearson, spearman = calculate_metrics(preds, np.array(y).flatten())\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Transfer Learning on TPOT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# read in data file\n",
    "data_folder = './output/'\n",
    "data_file = 'experimental_data_fineturn.csv'\n",
    "\n",
    "# give inputs for data generation\n",
    "input_col_name = 'seq'\n",
    "target_col = 'target'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'regression'\n",
    "model_type = 'tpot'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# Format data inputs appropriately for autoML platform\n",
    "transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "\n",
    "X = numerical_data_input\n",
    "y = transformed_output # don't convert to categorical for tpot\n",
    "X, y = reformat_data_traintest(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ckpt/rigorous/Gene_group1'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give inputs for paths\n",
    "final_model_path = f'{root_path}/outputs/tpot/regression/'\n",
    "final_model_name = 'final_model_tpot_regression.pkl'\n",
    "output_folder = final_model_path\n",
    "\n",
    "with open(final_model_path+final_model_name, 'rb') as file:  \n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model on new test data R2 :  -0.03797631535662571\n",
      "Original model on new test data:  (0.49189595318857504, 1.7510188135108316e-07)\n",
      "R2: -0.03797631535662571\n",
      "Pearson: 0.49189595318857504\n",
      "Spearman: 0.5182833962846561\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = model.predict(X)\n",
    "\n",
    "r2 = r2_score(np.array(y), preds)\n",
    "print('Original model on new test data R2 : ', r2)\n",
    "print('Original model on new test data: ', sp.pearsonr(y, preds))\n",
    "\n",
    "\n",
    "r2, pearson, spearman = calculate_metrics(preds, np.array(y).flatten())\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open('output/rigorous.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 转换字典为DataFrame\n",
    "data = pd.DataFrame.from_dict(data, orient='index', columns=['DeepSwarm', 'AutoKeras', 'Tpot'])\n",
    "metrics = [\"r2\", \"pearson\", \"spearman\"]\n",
    "\n",
    "# 创建一个Excel writer对象\n",
    "writer = pd.ExcelWriter('output/rigorous/rigorous_testset_results.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for i in range(3):\n",
    "   # 对每个指标进行处理\n",
    "   new_data = data.applymap(lambda x: x[i] if isinstance(x, list) else x)\n",
    "   new_data.index = [x.split(\"/\")[-1] for x in new_data.index]\n",
    "   \n",
    "   # 将处理后的数据写入不同的sheet\n",
    "   new_data.to_excel(writer, sheet_name=metrics[i])\n",
    "\n",
    "# 保存Excel文件\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
