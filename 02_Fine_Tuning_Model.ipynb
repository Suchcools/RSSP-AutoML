{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides code to test all models with validation datasets (either held out test sets or external validation datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "# import statements \n",
    "import sys\n",
    "sys.path.insert(1, './BioAutoMATED/main_classes/')\n",
    "sys.path.append('./BioAutoMATED')\n",
    "from wrapper import run_bioautomated\n",
    "from integrated_design_helpers import *\n",
    "from generic_automl_classes import convert_generic_input, read_in_data_file\n",
    "from generic_deepswarm import print_summary\n",
    "from transfer_learning_helpers import transform_classification_target, transform_regression_target, fit_final_deepswarm_model\n",
    "from generic_tpot import reformat_data_traintest\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.stats as sp\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import autokeras\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>Seq</th>\n",
       "      <th>OD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atoB(thl)_100</td>\n",
       "      <td>CGCGCCTTGACGGCTAGCTCAGTCCTAGGTATTGTGCTAGCCGTCG...</td>\n",
       "      <td>11.688265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atoB(thl)_130</td>\n",
       "      <td>CGCGCCAAAAAGAGTATTGACTTCGCATCTTTTTGTACCCATAATT...</td>\n",
       "      <td>12.008913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atoB(thl)_140</td>\n",
       "      <td>CGCGCCTTGACATAAAGTCTAACCTATAGGTATAATGTGTGGATCT...</td>\n",
       "      <td>9.565730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atoB(thl)_150</td>\n",
       "      <td>CGCGCCTTGACAATTAATCATCCGGCTCGTATAATGTGTGGAATTG...</td>\n",
       "      <td>11.556572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atoB(thl)_160</td>\n",
       "      <td>CGCGCCTTGACGGCTAGCTCAGTCCTAGGTACAGTGCTAGCTTAAT...</td>\n",
       "      <td>9.913603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>GFP_320</td>\n",
       "      <td>CGCGCCTTGACATTTATCCCTTGCGGCGATATAATGTGTGGATAAG...</td>\n",
       "      <td>10.105502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>GFP_40</td>\n",
       "      <td>CGCGCCTTGACATAAAGTCTAACCTATAGGCATAATTATTTCATCC...</td>\n",
       "      <td>9.076736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>GFP_50</td>\n",
       "      <td>CGCGCCTTGACAGCTAGCTCAGTCCTAGGTATAATGCTAGCACGAA...</td>\n",
       "      <td>8.622338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>GFP_70</td>\n",
       "      <td>CGCGCCAAAAAGAGTATTGACTTCGCATCTTTTTGTACCTATAATA...</td>\n",
       "      <td>9.005457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>GFP_90</td>\n",
       "      <td>CGCGCCTTGACATCGCATCTTTTTGTACCTATAATGTGTGGATAGA...</td>\n",
       "      <td>8.519434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Gene                                                Seq  \\\n",
       "0    atoB(thl)_100  CGCGCCTTGACGGCTAGCTCAGTCCTAGGTATTGTGCTAGCCGTCG...   \n",
       "1    atoB(thl)_130  CGCGCCAAAAAGAGTATTGACTTCGCATCTTTTTGTACCCATAATT...   \n",
       "2    atoB(thl)_140  CGCGCCTTGACATAAAGTCTAACCTATAGGTATAATGTGTGGATCT...   \n",
       "3    atoB(thl)_150  CGCGCCTTGACAATTAATCATCCGGCTCGTATAATGTGTGGAATTG...   \n",
       "4    atoB(thl)_160  CGCGCCTTGACGGCTAGCTCAGTCCTAGGTACAGTGCTAGCTTAAT...   \n",
       "..             ...                                                ...   \n",
       "562        GFP_320  CGCGCCTTGACATTTATCCCTTGCGGCGATATAATGTGTGGATAAG...   \n",
       "563         GFP_40  CGCGCCTTGACATAAAGTCTAACCTATAGGCATAATTATTTCATCC...   \n",
       "564         GFP_50  CGCGCCTTGACAGCTAGCTCAGTCCTAGGTATAATGCTAGCACGAA...   \n",
       "565         GFP_70  CGCGCCAAAAAGAGTATTGACTTCGCATCTTTTTGTACCTATAATA...   \n",
       "566         GFP_90  CGCGCCTTGACATCGCATCTTTTTGTACCTATAATGTGTGGATAGA...   \n",
       "\n",
       "            OD  \n",
       "0    11.688265  \n",
       "1    12.008913  \n",
       "2     9.565730  \n",
       "3    11.556572  \n",
       "4     9.913603  \n",
       "..         ...  \n",
       "562  10.105502  \n",
       "563   9.076736  \n",
       "564   8.622338  \n",
       "565   9.005457  \n",
       "566   8.519434  \n",
       "\n",
       "[567 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata = pd.read_csv('./AutoML_Data_Process/R_P_CDS.csv')\n",
    "rawdata.at[0, 'Seq'] = rawdata.iloc[0].Seq*5\n",
    "rawdata.Seq = rawdata.Seq.apply(lambda x:x[:826])\n",
    "rawdata.to_csv('./output/experimental_data_fineturn.csv', index=False, encoding = 'utf_8_sig')\n",
    "rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def calculate_metrics(preds, y):\n",
    "    \"\"\"\n",
    "    Calculate 'R2', 'Pearson', and 'Spearman' metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - preds: Predicted values\n",
    "    - y: True values\n",
    "\n",
    "    Returns:\n",
    "    - r2: R-squared (R2) score\n",
    "    - pearson: Pearson correlation coefficient\n",
    "    - spearman: Spearman rank correlation coefficient\n",
    "    \"\"\"\n",
    "    # R-squared (R2) score\n",
    "    r2 = r2_score(y, preds)\n",
    "\n",
    "    # Pearson correlation coefficient\n",
    "    pearson, _ = pearsonr(preds, y)\n",
    "\n",
    "    # Spearman rank correlation coefficient\n",
    "    spearman, _ = spearmanr(preds, y)\n",
    "\n",
    "    return r2, pearson, spearman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Transfer Learning on a DeepSwarm Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "1692707059.049456 (InputLaye (None, 826, 4, 1)         0         \n",
      "_________________________________________________________________\n",
      "1692707059.0506542 (Conv2D)  (None, 826, 4, 64)        640       \n",
      "_________________________________________________________________\n",
      "1692707059.068253 (Conv2D)   (None, 826, 4, 8)         12808     \n",
      "_________________________________________________________________\n",
      "1692707059.082017 (Flatten)  (None, 26432)             0         \n",
      "_________________________________________________________________\n",
      "1692707059.0864587 (Dense)   (None, 1)                 26433     \n",
      "=================================================================\n",
      "Total params: 39,881\n",
      "Trainable params: 39,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "model is originally trainable: True\n",
      "number of layers in the model: 5\n",
      "0: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f85242d1290>, setting trainable to False\n",
      "1: <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f8524382a50>, setting trainable to False\n",
      "2: <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f85242d1810>, setting trainable to False\n",
      "3: <tensorflow.python.keras.layers.core.Flatten object at 0x7f85242d1690>, keeping trainable = True\n",
      "4: <tensorflow.python.keras.layers.core.Dense object at 0x7f85242d1650>, keeping trainable = True\n"
     ]
    }
   ],
   "source": [
    "# Load DeepSwarm Model and freeze all except last two layers (randomly chose this - feel free to customize)\n",
    "final_model_path = './BioAutoMATED/literature/outputs/deepswarm/regression/'\n",
    "final_model_name = 'deepswarm_deploy_model.h5'\n",
    "# get sequences with help from https://stackoverflow.com/questions/53183865/unknown-initializer-glorotuniform-when-loading-keras-model\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform(), 'BatchNormalizationV1': BatchNormalization()}): # , 'BatchNormalizationV1': BatchNormalization()\n",
    "    model = tf.keras.models.load_model(final_model_path + final_model_name)\n",
    "print(model.summary())\n",
    "print('model is originally trainable: ' + str(model.trainable))\n",
    "print('number of layers in the model: ' + str(len(model.layers)))\n",
    "\n",
    "# set all layers except last two dense ones to be fixed\n",
    "for layer_idx, layer in enumerate(model.layers):\n",
    "    if layer_idx > len(model.layers) - 3:\n",
    "        print(str(layer_idx) + ': ' + str(layer) + ', keeping trainable = ' + str(layer.trainable))\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        print(str(layer_idx) + ': ' + str(layer) + ', setting trainable to ' + str(layer.trainable))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 微调前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Padding all sequences to a length of 826\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# Transform the test set RBS data to fine-tune this model\n",
    "data_folder = './output/'\n",
    "data_file = 'experimental_data_fineturn.csv'\n",
    "\n",
    "# Give inputs for data generation\n",
    "input_col = 'Seq'\n",
    "target_col = 'OD'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'regression'\n",
    "model_type = 'deepswarm'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# transform output (target) into bins for classification\n",
    "transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "    \n",
    "# now, we have completed the pre-processing needed to feed our data into deepswarm\n",
    "# deepswarm input: numerical_data_input\n",
    "# deepswarm output: transformed_output\n",
    "X = numerical_data_input\n",
    "y = transformed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -1.319840171329898\n",
      "Pearson: [0.01151065]\n",
      "Spearman: 0.002300183888643859\n"
     ]
    }
   ],
   "source": [
    "# 使用微调前的模型进行预测\n",
    "preds = model.predict(X)\n",
    "r2, pearson, spearman = calculate_metrics(preds, y)\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 微调后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Padding all sequences to a length of 826\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# Transform the test set RBS data to fine-tune this model\n",
    "data_folder = './output/'\n",
    "data_file = 'experimental_data_fineturn.csv'\n",
    "\n",
    "# Give inputs for data generation\n",
    "input_col = 'Seq'\n",
    "target_col = 'OD'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'regression'\n",
    "model_type = 'deepswarm'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# transform output (target) into bins for classification\n",
    "transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "    \n",
    "# now, we have completed the pre-processing needed to feed our data into deepswarm\n",
    "# deepswarm input: numerical_data_input\n",
    "# deepswarm output: transformed_output\n",
    "X = numerical_data_input\n",
    "y = transformed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (453, 826, 4, 1)\n",
      "y_train shape: (453, 1)\n",
      "X_test shape: (114, 826, 4, 1)\n",
      "y_test shape: (114, 1)\n"
     ]
    }
   ],
   "source": [
    "# 数据集大小\n",
    "total_samples = X.shape[0]\n",
    "train_samples = int(0.8 * total_samples)\n",
    "test_samples = total_samples - train_samples\n",
    "\n",
    "# 随机打乱数据索引\n",
    "indices = np.arange(total_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# 根据8:2划分比例获取训练集和测试集的索引\n",
    "train_indices = indices[:train_samples]\n",
    "test_indices = indices[train_samples:]\n",
    "\n",
    "# 根据索引划分数据集\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "# 打印数据集形状\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final model now...\n",
      "Train on 407 samples, validate on 46 samples\n",
      "Epoch 1/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.4568 - mean_squared_error: 0.4568 - val_loss: 0.2800 - val_mean_squared_error: 0.2800\n",
      "Epoch 2/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.3285 - mean_squared_error: 0.3285 - val_loss: 0.2813 - val_mean_squared_error: 0.2813\n",
      "Epoch 3/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.2836 - mean_squared_error: 0.2836 - val_loss: 0.2447 - val_mean_squared_error: 0.2447\n",
      "Epoch 4/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.2416 - mean_squared_error: 0.2416 - val_loss: 0.2124 - val_mean_squared_error: 0.2124\n",
      "Epoch 5/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.2066 - mean_squared_error: 0.2066 - val_loss: 0.2010 - val_mean_squared_error: 0.2010\n",
      "Epoch 6/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.1841 - mean_squared_error: 0.1841 - val_loss: 0.1955 - val_mean_squared_error: 0.1955\n",
      "Epoch 7/30\n",
      "407/407 [==============================] - 1s 2ms/sample - loss: 0.1664 - mean_squared_error: 0.1664 - val_loss: 0.1920 - val_mean_squared_error: 0.1920\n",
      "Epoch 8/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.1492 - mean_squared_error: 0.1492 - val_loss: 0.1884 - val_mean_squared_error: 0.1884\n",
      "Epoch 9/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.1383 - mean_squared_error: 0.1383 - val_loss: 0.1886 - val_mean_squared_error: 0.1886\n",
      "Epoch 10/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.1298 - mean_squared_error: 0.1298 - val_loss: 0.1861 - val_mean_squared_error: 0.1861\n",
      "Epoch 11/30\n",
      "407/407 [==============================] - 1s 2ms/sample - loss: 0.1175 - mean_squared_error: 0.1175 - val_loss: 0.1839 - val_mean_squared_error: 0.1839\n",
      "Epoch 12/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.1104 - mean_squared_error: 0.1104 - val_loss: 0.1852 - val_mean_squared_error: 0.1852\n",
      "Epoch 13/30\n",
      "407/407 [==============================] - 1s 2ms/sample - loss: 0.1042 - mean_squared_error: 0.1042 - val_loss: 0.1837 - val_mean_squared_error: 0.1837\n",
      "Epoch 14/30\n",
      "407/407 [==============================] - 1s 2ms/sample - loss: 0.0960 - mean_squared_error: 0.0960 - val_loss: 0.1943 - val_mean_squared_error: 0.1943\n",
      "Epoch 15/30\n",
      "407/407 [==============================] - 1s 3ms/sample - loss: 0.0967 - mean_squared_error: 0.0967 - val_loss: 0.1931 - val_mean_squared_error: 0.1931\n",
      "Epoch 16/30\n",
      "407/407 [==============================] - 1s 2ms/sample - loss: 0.0882 - mean_squared_error: 0.0882 - val_loss: 0.1863 - val_mean_squared_error: 0.1863\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "1692707059.049456 (InputLaye (None, 826, 4, 1)         0         \n",
      "_________________________________________________________________\n",
      "1692707059.0506542 (Conv2D)  (None, 826, 4, 64)        640       \n",
      "_________________________________________________________________\n",
      "1692707059.068253 (Conv2D)   (None, 826, 4, 8)         12808     \n",
      "_________________________________________________________________\n",
      "1692707059.082017 (Flatten)  (None, 26432)             0         \n",
      "_________________________________________________________________\n",
      "1692707059.0864587 (Dense)   (None, 1)                 26433     \n",
      "=================================================================\n",
      "Total params: 39,881\n",
      "Trainable params: 26,433\n",
      "Non-trainable params: 13,448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "finetune_model_name = 'fine_tune_deepswarm_deploy_model.h5'\n",
    "    \n",
    "print('Fitting final model now...')\n",
    "num_epochs = 30 # can choose how many epochs you want\n",
    "deploy_model = fit_final_deepswarm_model(model, task, num_epochs,  X_train, y_train)\n",
    "        \n",
    "# Save the final deploy trained model\n",
    "deploy_model.save(final_model_path + finetune_model_name)\n",
    "print_summary(deploy_model, final_model_path + 'fine_tune_model_topology.txt')\n",
    "print(deploy_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.4511488891585621\n",
      "Pearson: [0.69088473]\n",
      "Spearman: 0.7262023855328974\n"
     ]
    }
   ],
   "source": [
    "# 使用微调前的模型进行预测\n",
    "preds = deploy_model.predict(X_test)\n",
    "r2, pearson, spearman = calculate_metrics(preds, y_test)\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Transfer Learning on an AutoKeras Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Padding all sequences to a length of 826\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "data_folder = './output/'\n",
    "data_file = 'experimental_data_fineturn.csv'\n",
    "\n",
    "\n",
    "# Give inputs for data generation\n",
    "input_col = 'Seq'\n",
    "target_col = 'OD'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'regression'\n",
    "model_type = 'autokeras'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# Format data inputs appropriately for autoML platform\n",
    "transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "\n",
    "# now, we have completed the pre-processing needed to feed our data into autokeras\n",
    "# autokeras input: oh_data_input\n",
    "# autokeras output: transformed_output\n",
    "X = oh_data_input\n",
    "y = transformed_output # don't convert to categorical for autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = './BioAutoMATED/literature/models/autokeras/regression/'\n",
    "final_model_name = 'optimized_autokeras_pipeline_regression.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared after no retraining:  -0.08753596947344922\n",
      "Evaluation after no retraining:  0.3797411622989434\n",
      "R2: -0.08753596947344922\n",
      "Pearson: 0.1878518761272091\n",
      "Spearman: 0.15101655738478229\n",
      "R-squared after some retraining:  0.20879726410383748\n",
      "Evaluation after some retraining:  0.27626879016128747\n",
      "R2: 0.20879726410383748\n",
      "Pearson: 0.5160097091631651\n",
      "Spearman: 0.5120713241190622\n",
      "R-squared after training weights from scratch:  0.3874438134200159\n",
      "Evaluation after training weights from scratch:  0.21388975150671605\n",
      "R2: 0.3874438134200159\n",
      "Pearson: 0.6349562184367432\n",
      "Spearman: 0.6535874333694985\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.85\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, np.array(y).astype(float),train_size=train_size, test_size = 1-train_size)\n",
    "\n",
    "clf = autokeras.utils.pickle_from_file(final_model_path+final_model_name)\n",
    "\n",
    "evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "\n",
    "preds = clf.predict(np.array(X_test_new))\n",
    "r2 = r2_score(np.array(y_test_new), preds)\n",
    "print(\"R-squared after no retraining: \", r2)\n",
    "print('Evaluation after no retraining: ', evaluation)\n",
    "r2, pearson, spearman = calculate_metrics(preds, np.array(y_test_new).flatten())\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")\n",
    "\n",
    "\n",
    "# retrain = False indicates that the weights should be reused and then retrained\n",
    "# retrain = True indicates that the weights should be reinitialized from scratch\n",
    "# this may seem unintuitive but in the documentation, retrain is a boolean indicating whether or not to reinitialize the weights of the model\n",
    "clf.fit(np.array(X_train_new),np.array(y_train_new), retrain=False)\n",
    "evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "preds = clf.predict(np.array(X_test_new))\n",
    "r2 = r2_score(np.array(y_test_new), preds)\n",
    "print(\"R-squared after some retraining: \", r2)\n",
    "print('Evaluation after some retraining: ', evaluation)\n",
    "r2, pearson, spearman = calculate_metrics(preds, np.array(y_test_new).flatten())\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")\n",
    "\n",
    "\n",
    "# can save and reload at will\n",
    "autokeras.utils.pickle_to_file(clf, final_model_path + 'fine_tune_autokeras_pipeline_classification.h5')\n",
    "test = autokeras.utils.pickle_from_file(final_model_path+'fine_tune_autokeras_pipeline_classification.h5')\n",
    "\n",
    "# showing retrain = True wipes the old weights and ends up with a worse model\n",
    "clf.fit(np.array(X_train_new),np.array(y_train_new), retrain=True)\n",
    "evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "preds = clf.predict(np.array(X_test_new))\n",
    "r2 = r2_score(np.array(y_test_new), preds)\n",
    "print(\"R-squared after training weights from scratch: \", r2)\n",
    "print('Evaluation after training weights from scratch: ', evaluation)\n",
    "\n",
    "r2, pearson, spearman = calculate_metrics(preds, np.array(y_test_new).flatten())\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Transfer Learning on TPOT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Padding all sequences to a length of 826\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# read in data file\n",
    "data_folder = './output/'\n",
    "data_file = 'experimental_data_fineturn.csv'\n",
    "\n",
    "# give inputs for data generation\n",
    "input_col_name = 'Seq'\n",
    "target_col = 'OD'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'regression'\n",
    "model_type = 'tpot'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# Format data inputs appropriately for autoML platform\n",
    "transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "\n",
    "X = numerical_data_input\n",
    "y = transformed_output # don't convert to categorical for tpot\n",
    "training_features, training_target = reformat_data_traintest(X, y)\n",
    "train_size = 0.85\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(training_features, training_target, train_size=train_size, test_size = 1-train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give inputs for paths\n",
    "final_model_path = './BioAutoMATED/literature/outputs/tpot/regression/'\n",
    "final_model_name = 'final_model_tpot_regression.pkl'\n",
    "output_folder = final_model_path\n",
    "\n",
    "with open(final_model_path+final_model_name, 'rb') as file:  \n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No partial_fit could be applied. Trying warm_start instead.\n",
      "\n",
      "Original model on new test data R2 :  -0.32680605057210044\n",
      "Original model on new test data:  (0.25109081649536685, 0.019703911226954935)\n",
      "R2: -0.32680605057210044\n",
      "Pearson: 0.25109081649536685\n",
      "Spearman: 0.2594367658851832\n",
      "Keys that must be manually changed in the model to allow fine-tuning on new data: \n",
      "\tn_estimators\n",
      "\twarm_start\n",
      "Fine-tuned model on new test data R2 :  0.28544609526852893\n",
      "Fine-tuned model on new test data:  (0.5834482839037382, 3.738224773868991e-09)\n",
      "R2: 0.28544609526852893\n",
      "Pearson: 0.5834482839037382\n",
      "Spearman: 0.605207792820416\n"
     ]
    }
   ],
   "source": [
    "# partial_fit transfer learning is only possible for models that support it - most do not\n",
    "# see reference list of those models here: https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning\n",
    "try:\n",
    "    model.partial_fit(X_train_new,y_train_new)\n",
    "except:\n",
    "    print(\"No partial_fit could be applied. Trying warm_start instead.\")\n",
    "    print(\"\")\n",
    "try:\n",
    "    preds = model.predict(X_test_new)\n",
    "    \n",
    "    r2 = r2_score(np.array(y_test_new), preds)\n",
    "    print('Original model on new test data R2 : ', r2)\n",
    "    print('Original model on new test data: ', sp.pearsonr(y_test_new, preds))\n",
    "    \n",
    "    \n",
    "    r2, pearson, spearman = calculate_metrics(preds, np.array(y_test_new).flatten())\n",
    "    print(f\"R2: {r2}\")\n",
    "    print(f\"Pearson: {pearson}\")\n",
    "    print(f\"Spearman: {spearman}\")\n",
    "    \n",
    "    print('Keys that must be manually changed in the model to allow fine-tuning on new data: ')\n",
    "    for key in list(model.get_params().keys()):\n",
    "        if 'warm_start' in key or 'n_estimator' in key:\n",
    "            print('\\t' + key)\n",
    "        model.set_params(warm_start = True)\n",
    "        model.set_params(n_estimators = 1 + model.get_params()['n_estimators'])\n",
    "\n",
    "    model.fit(X_train_new,y_train_new)\n",
    "    preds = model.predict(X_test_new)\n",
    "    r2 = r2_score(np.array(y_test_new), preds)\n",
    "    print('Fine-tuned model on new test data R2 : ', r2)    \n",
    "    print('Fine-tuned model on new test data: ', sp.pearsonr(y_test_new, preds))\n",
    "    r2, pearson, spearman = calculate_metrics(preds, np.array(y_test_new).flatten())\n",
    "    print(f\"R2: {r2}\")\n",
    "    print(f\"Pearson: {pearson}\")\n",
    "    print(f\"Spearman: {spearman}\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"No warm_start could be applied. Model is not compatible with transfer learning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Transfer Learning on AutoKeras Toehold Regression Model + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'regression'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "model_type = 'autokeras'\n",
    "final_model_path = './BioAutoMATED/literature/models/autokeras/regression/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With retrained Green et al. models, predict on test sets (Pardee et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------  ROUND 0 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Padding all sequences to a length of 826\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "pearsons = []\n",
    "spearmans = []\n",
    "R2s = []\n",
    "for i in range(25): # do any # of trials\n",
    "    print(\"--------------  ROUND \" + str(i) + \" --------------\")\n",
    "    # Read in Green et al. data file\n",
    "    data_folder = './output/'\n",
    "    data_file = 'experimental_data_fineturn.csv'\n",
    "\n",
    "    # Give inputs for data generation\n",
    "    input_col = 'seq'\n",
    "    target_col = 'target'\n",
    "    final_model_name = 'optimized_autokeras_pipeline_regression.h5'\n",
    "\n",
    "    # allows user to interpret model with data not in the original training set\n",
    "    # so apply typical cleaning pipeline\n",
    "    df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "\n",
    "    # Format data inputs appropriately for autoML platform\n",
    "    numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "    transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "\n",
    "    # now, we have completed the pre-processing needed to feed our data into autokeras\n",
    "    # autokeras input: oh_data_input\n",
    "    # autokeras output: transformed_output\n",
    "    X = oh_data_input\n",
    "    y = transformed_output # don't convert to categorical for autokeras\n",
    "\n",
    "    train_size = 0.9\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, np.array(y).astype(float),train_size=train_size, test_size = 1-train_size)\n",
    "\n",
    "    clf = autokeras.utils.pickle_from_file(final_model_path+final_model_name)\n",
    "    evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "\n",
    "    # retrain = False indicates that the weights should be reused and then retrained\n",
    "    # retrain = True indicates that the weights should be reinitialized from scratch\n",
    "    clf.fit(np.array(X_train_new),np.array(y_train_new), retrain=False)\n",
    "    evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "    y_pred = clf.predict(np.array(X_test_new))\n",
    "    r2 = r2_score(np.array(y_test_new), y_pred)\n",
    "    R2s.append(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6487315269228773\n",
      "0.016604972636010205\n",
      "0.6456944583216552\n",
      "0.016758170318431506\n",
      "-47.93336678706019\n",
      "0.28280402583622866\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(pearsons))\n",
    "print(np.std(pearsons))\n",
    "print(np.mean(spearmans))\n",
    "print(np.std(spearmans))\n",
    "print(np.mean(R2s))\n",
    "print(np.std(R2s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
